## 全 0
1. 在线性回归中，基本上采用此方法
2. 在神经网络中不能使用，因为在正向传播时，每层的所有神经元都会输出相同的值。这意味着在反向传播时，所有权重的梯度也将相同，导致所有权重以完全相同的方式更新
## 随机
1. 将权重初始化为小的随机数。通常是从一个均值为 0、标准差很小的正态分布或均匀分布中取样
2. 如果随机数过大或过小，仍然可能导致梯度消失或梯度爆炸
## Xavier
1. 专门为 Sigmoid 和 Tanh 等激活函数设计，核心思想为保持每一层激活值的方差不变，尽可能让输入和输出服从相同的分布
2. 从以下正态分布或均匀分布中取样，$n _{in}$ 是输入神经元的数量
$$W \sim U(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})$$
$$W \sim N(0, \sqrt{\frac{2}{n_{in} + n_{out}}})$$
3. 对于目前神经网络中最常用的 ReLU 激活函数不适用
## He
1. 专门为 ReLU 及其变体（如 Leaky ReLU）设计
2. 从以下正态分布或均匀分布中取样
$$W \sim U(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})$$
$$W \sim N(0, \sqrt{\frac{2}{n_{in}}})$$