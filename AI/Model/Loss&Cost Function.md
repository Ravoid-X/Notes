## 损失函数
通常是针对单个训练样本而言，分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。
### 0-1
$$L(Y, f(\mathbf{x})) = \begin{cases} 1, & \text{if } Y \neq f(\mathbf{x}) \\ 0, & \text{if } Y = f(\mathbf{x}) \end{cases}$$
1. 常用于分类问题，感知机就是用的这种损失函数，$|Y-f(x)|<T$ 可认为相等
2. 优点：简单直观，直接对应分类的准确率。
3. 缺点：它是一个非凸函数，无法求导，因此在训练过程中无法用于梯度下降，主要用于模型评估而非训练。
### 绝对值
$$L(y, \hat{y}) = |y - \hat{y}|$$
1. 优点：对离群点（Outliers）不那么敏感，因为惩罚是线性的，而非平方。
2. 缺点：在 0 点处不可导，这可能会对梯度下降优化造成一些挑战。
### 平方
$$L(y, \hat{y}) = (y - \hat{y})^2$$
1. 常用于回归问题
2. 优点：可导，计算简单。
3. 缺点：对离群点过于敏感，可能导致模型过度关注这些异常值。
### Log（交叉熵损失）
$$L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]$$
$$L = -\sum_{j=1}^{C} y_j \log(\hat{y}_j)//多分类$$
$y _j$ 是指示变量（当真实类别为 j 时为 1，否则为 0）
1. 也称为交叉熵损失，是分类问题中最常用的损失函数，尤其适用于概率预测
2. 优点：能够有效衡量预测概率的优劣，可导，适合用于梯度下降优化。
3. 缺点：对输入为 0 的预测概率非常敏感，需要平滑处理。
### 指数
$$L(y, f(\mathbf{x})) = e^{-y \cdot f(\mathbf{x})}$$
1. 对预测错误的样本给予指数级惩罚，常用于Adaboost等集成学习算法中
2. 优点：对错误分类的样本有很强的惩罚，能够迅速纠正模型的错误。
3. 缺点：对离群点非常敏感，容易导致模型过拟合。
### Hinge 
$$L(y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})$$
1. 主要用于最大间隔分类器，如支持向量机（SVM）
2. 不仅惩罚错误分类的样本，还会惩罚那些虽然分类正确但离决策边界太近的样本。目标是找到一个足够大的“间隔”。
3. 优点：使模型具有很强的泛化能力，能找到最优的决策边界
4. 在 $y \cdot \hat{y}=1$ 处不可导。
### 感知机（Perceptron Loss）
$$L(y, f(\mathbf{x})) = \max(0, -y \cdot f(\mathbf{x}))$$
1. 是Hinge损失函数的一个变种，只惩罚被错误分类的样本
2. 缺点：是一个非凸函数，且在正确分类的样本上损失为 0，可能导致模型无法找到最佳决策边界。
## 代价函数
衡量模型在整个训练集或一个训练批次上的平均表现的指标
### 均方误差（Mean Squared Error, MSE）
$$J(\mathbf{w}, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
1. 回归任务中最常用的代价函数。在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的，因此在这个假设能被满足的场景中（比如回归）是一个很好的选择；当没能被满足（比如分类），不是一个好的选择。
2. 优点：计算简单，可导，对大误差的惩罚更大。
3. 缺点：对离群点非常敏感，因为误差被平方，离群点的影响会被放大。
### 平均绝对误差（Mean Absolute Error, MAE）
$$J(\mathbf{w}, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$
1. 同样用于回归任务，通常比 MSW 更慢收敛，但更加不易受到离群点影响
2. 缺点：在误差为 0 的点处不可导
### Huber 
$$L_{\delta}(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \le \delta \\ \delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{otherwise} \end{cases}$$
1. 优点：结合了 MSE 的可导性和 MAE 对离群点的鲁棒性，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低离群点的影响
### 分位数（Quantile Loss）
$$L_{\gamma}(y, \hat{y}) = \begin{cases} \gamma(y - \hat{y}) & \text{if } y > \hat{y} \\ (1 - \gamma)(\hat{y} - y) & \text{if } y \le \hat{y} \end{cases}$$
1. $\gamma$ 是一个超参数，代表我们想要预测的分位数。它的取值范围是 (0,1)。
2. 当 $\gamma=0.5$ 时，分位数损失退化为平均绝对误差（MAE），因为此时对正负误差的惩罚是相等的，目标是预测中位数。
3. 当 $\gamma>0.5$ 时，模型会更倾向于高估预测值，因为它对负误差（预测值大于真实值）的惩罚更小。
4. 当 $\gamma<0.5$ 时，模型会更倾向于低估预测值，因为它对正误差（预测值小于真实值）的惩罚更小。
### 交叉熵（二分类）
$$J(\mathbf{w}, \mathbf{b}) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$
1. 分类任务中最常用的代价函数
2. 优点：能够有效衡量预测概率的优劣，可导，是分类任务的首选。
3. 缺点：如果预测概率为 0，损失会变为无穷大，因此需要进行平滑处理。
### Hinge
$$J(\mathbf{w}, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)$$
1. 主要用于最大间隔分类器，如支持向量机（SVM）
2. 优点：使模型具有很强的泛化能力，能找到最优的决策边界
3. 在 $y \cdot \hat{y}=1$ 处不可导。