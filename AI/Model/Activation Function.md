为了引入非线性
## Sigmoid
$$ S(x) = \frac{1}{1 + e^{-x}} $$
### 优点
输出值范围在 0 到 1 之间，可以被解释为概率，因此在早期常用于二分类任务的输出层
### 缺点
1. 当输入值 x 很大或很小时，函数的导数会趋近于 0，可能引发梯度消失
2. 输出不是以 0 为中心，这会影响后续层的学习
## Tanh
$$ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$
### 优点
1. 输出以 0 为中心，通常收敛速度比 Sigmoid 更快
### 缺点
1. 依然存在梯度消失问题
## ReLU
$$ \text{ReLU}(x) = \max(0, x) $$
### 优点
1. x>0 时导数恒为 1，有效缓解了梯度消失问题。
2. 只需要进行简单的判断和比较，计算速度比 Sigmoid 和 Tanh 快得多
### 缺点
1. 如果一个神经元在训练过程中始终只接收到负数输入，那么它的梯度将永远为 0，导致该神经元不再更新
## Leaky ReLU
$$ \text{Leaky ReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$
$\alpha$ 是一个很小的正数，通常为 0.01
### 优点
1. 在负数区间导数不为 0，总能获得梯度。
### 缺点
1. 性能不总是优于 ReLU，需要手动设置 $\alpha$ 这个超参数