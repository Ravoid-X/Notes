## 连续特征离散化
也称为分箱（Binning）
### 目的
1. 降低维度：将无限的连续取值压缩成有限的离散类别，简化了特征空间。
2. 处理非线性关系：某些模型（如线性回归）难以直接捕捉特征与目标之间的非线性关系。离散化可以帮助将这种非线性关系转化为阶梯状的线性关系，使模型更容易学习。
3. 提高模型鲁棒性：对数据中的微小波动和异常值不敏感，能增强模型的泛化能力，比如一个特征是年龄>30是1，否则0。没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
4. 适配特定模型：某些模型，如朴素贝叶斯或决策树，天然适合处理离散特征。
### 基于阈值/分位数
比如根据四分位数（25%、50%、75%），作为切分点，将数据划分为不同的区间。
### 基于模型（如 GBDT）
1. 训练 GBDT 模型：GBDT 的每棵决策树都会根据数据中的最佳信息增益来确定切分点。
2. 提取离散特征：模型训练完成后使用 apply 方法。会将每个样本映射到每棵树的叶子节点上。
3. 生成独热编码：每个叶子节点可以看作是一个离散的类别。一个样本在每棵树上都会落在一个特定的叶子节点，用独热编码来表示这个叶子节点。
## 离散特征连续化
许多机器学习模型，特别是基于距离或梯度的模型（如SVM、逻辑回归、神经网络等），要求输入特征是连续的数值。将离散特征转换为连续的数值形式称为特征编码（Feature Encoding）
### 独热编码
最常用直观的方法，核心思想是将每一个离散特征的可能取值都看作一个独立的二元特征。
1. 适用于特征值较少的情况
2. 优点：简单且易于理解，避免了模型误认为特征之间存在顺序关系
3. 缺点：离散特征取值非常多时，会创建大量的稀疏特征，导致数据维度急剧增加，不仅增加计算和存储成本，还可能导致模型训练困难；无法捕捉特征之间的相似性。
### 标签编码
1. 将离散特征直接映射为整数的方法，例如“城市”特征有三个值：'北京', '上海', '深圳'，可以分别编码为 0, 1, 2。
2. 优点：简单且节省空间，不会增加特征维度。
3. 缺点：引入了人为的顺序关系，仅适用于有内在顺序的离散特征，例如“学历”特征：['小学', '中学', '大学']。
### 特征嵌入
核心思想是用一个可训练的向量来表示每个离散特征的值，将高维、稀疏的离散特征映射到低维、稠密的向量空间。
1. 步骤\
（1）标签编码：将离散特征每个唯一值映射为一个唯一整数。例如“城市”特征有三个值：'北京', '上海', '深圳'，可以分别编码为 0, 1, 2。\
（2）在神经网络中，创建一个嵌入层。本质上是一个查找表（lookup table），维度是 [特征值的数量, 嵌入向量的维度]。比如“城市”特征有 100 个唯一值，选择用一个 16 维的向量来表示，那么嵌入层的大小就是 [100, 16]。\
（3）前向传播：样本特征的整数索引作为输入，嵌入层查找并返回对应的嵌入向量。这个向量是一个稠密的、浮点数的数组。\
（4）联合训练：嵌入层作为神经网络的一部分，与其他层一起进行端到端的训练，并在反向传播的迭代中优化。
2. 应用于具有大量离散特征的大规模数据，如用户ID、商品ID、电影ID
### 特征哈希
1. 工作原理\
（1）设置一个固定维度的哈希表（通常是数组）。\
（2）对每个离散特征的取值，应用一个哈希函数，将它映射到一个整数索引。\
（3）将这个整数索引作为新特征的下标，并在对应位置上进行累加（通常为1）。
2. 优点：无需存储词汇表，节省内存；维度固定，避免了维度灾难；计算效率高。
3. 缺点：不同的原始特征值可能被哈希到同一个索引上，导致信息损失；编码完成后，无法从哈希后的数值还原出原始特征值
## 离散特征离散化
### 独热编码、标签编码
如上
### 虚拟编码
与独热编码类似，区别是如果特征有 N 个取值，它只需要 N-1 个新的二元特征来代替
### 基于业务逻辑的归并
1. 如可以将“春、夏、秋、冬”这四个季节合并为“淡季、旺季”两个类别。
2. 优点：高度可解释，归并后的特征具有实际意义；减少了特征维度；捕捉非线性关系。
3. 缺点：需要人工经验；不具备通用性。