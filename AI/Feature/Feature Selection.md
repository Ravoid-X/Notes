## 过滤法（Filter Methods）
计算特征与目标变量之间的相关性或统计量，来评估每个特征的重要性，然后根据分数对特征进行排序和选择。
### 方差过滤（Variance Thresholding）
1. 如果特征的方差很低，说明取值几乎没有变化，对模型区分样本的贡献很小，可以被移除。
2. 通常作为最简单的预处理步骤，用于快速移除那些没有信息量的特征。
3. 设定一个方差阈值，移除方差低于该阈值的所有特征。
### 相关性分析（Correlation Analysis）
1. 皮尔逊相关系数（Pearson Correlation）\
（1）衡量两个连续变量之间的线性相关性，取值范围在 [−1,1]。\
（2）$r>0$ 表示正相关，$r<0$ 表示负相关，$r=0$ 表示无线性相关。$|r|$ 越大，相关性越强。\
（3）对异常值敏感，一个极端值可能大幅影响结果
2. 斯皮尔曼等级相关系数（Spearman's Rank Correlation）\
（1）衡量两个变量之间的单调关系，可以是非线性的。\
（2）将两个变量的原始数据分别进行排序，并用其排名代替原始数值\
（3）取值范围在 [−1,1]，$r>0$ 表示正相关，$r<0$ 表示负相关，$r=0$ 表示无线性相关
3. 卡方检验（Chi-squared Test）\
（1）用于衡量离散特征和离散目标变量之间的相关性。\
（2）基于观察频数和期望频数之间的差异，来判断两个变量是否独立。卡方值越大，说明两个变量越不独立，相关性越强。\
（3）适用于特征和目标变量都是类别型数据
### 互信息（Mutual Information）
1. 衡量任意两个变量之间的依赖性，可以捕捉线性和非线性的关系
2. 适用于特征与目标变量的关系是非线性的，或者数据类型混合（连续和离散）
### 优点
1. 计算速度快：独立于模型，计算开销小。
2. 通用性强：可以作为数据预处理步骤，适用于任何模型。
### 缺点
1. 不考虑特征之间的组合作用：它独立评估每个特征，可能忽略了某些特征组合起来后才显现的重要性
2. 不关心特征对于具体模型的预测能力，筛选出的“好”特征在某些模型上可能表现不佳。
## 包装法（Wrapper Methods）
将特征选择的过程看作一个优化问题，与过滤法不同，包装法不独立于模型，而是将其作为“黑盒”，通过不断地增减特征，来评估不同特征子集对模型性能的影响。
### 前向选择
从一个空的特征集开始，逐步添加能使模型性能提升最大的特征
1. 步骤\
（1）初始化一个空特征集 $S={}$\
（2）迭代：从未被选中的特征中，选择一个特征 $x _i$，将其添加到 S 中，然后训练模型并评估性能。\
（3）选择：保留使模型性能提升最大的那个特征，然后重复第2步，直到模型性能不再提升
2. 优点：简单直观；在特征数量不多时，效果通常很好。
3. 缺点：计算开销大，每一步都需要重新训练模型；因为贪心算法，可能无法找到全局最优的特征组合。
### 后向选择
与前向选择相反，从包含所有特征的集合开始，逐步移除对模型性能影响最小的特征。
1. 步骤\
（1）初始化一个包含所有特征的集合 $S _{all}$\
（2）迭代：从 $S _{all}$ 中移除一个特征 $x _i$，然后训练模型并评估性能\
（3）选择：移除使模型性能下降最小的特征，然后重复第2步，直到模型性能开始显著下降
2. 优缺点与前向选择相同
### 递归特征消除 (Recursive Feature Elimination, RFE)
特殊的后向消除，基于模型的特征重要性或系数来递归地移除最不重要的特征
1. 步骤\
（1）训练一个模型（如线性回归、决策树等），根据其权重或特征重要性对所有特征进行排序\
（2）移除排名最靠后的特征\
（3）重复上述过程，直到达到预设的特征数量
2. 优点：比传统的后向消除更高效；能够有效地找到一个最优的特征子集。
3. 缺点：依赖于特定模型的评估，不同模型对特征重要性的定义不同；无法捕捉到特征组合的复杂关系。
## 嵌入法（Embedded Methods）
将特征选择过程与模型训练过程集成在一起，不像过滤法那样独立于模型，也不像包装法那样需要反复地训练和评估模型。在模型训练过程中，通过优化目标函数，自动地对特征进行“打分”或“剪枝”，从而识别出对模型性能贡献最大的特征。
### 基于正则
#### L1正则
$$Loss = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p}|\beta_j|$$
1. 训练结束后查看模型的系数。系数不为零的特征就是被 Lasso 选出的重要特征。
2. 优点：能够同时进行特征选择和模型训练；得到的模型通常更简单、更易于解释。
3. 缺点：只适用于线性模型，如线性回归或逻辑回归；当特征之间存在高度相关性时，Lasso 倾向于选择其中一个特征，而忽略其他。
#### L2正则
### 基于树模型
1. 原理：许多基于树的集成学习模型（如随机森林、GBDT、XGBoost）在构建决策树时，会根据某个指标（如信息增益、基尼不纯度减少量）来选择最佳的特征进行分裂。一个特征在树中被用作分裂点的次数越多、分裂后带来的增益越大，它的重要性就越高。
2. 优点\
（1）能够捕捉非线性关系和特征之间的交互作用。\
（2）计算效率高，在训练模型的同时完成了特征选择。\
（3）效果通常非常好，是实践中最常用的方法之一。
3. 缺点：重要性分数只能反映特征在模型中的相对贡献，并不总是等同于其在实际问题中的重要性。
## 寻找高级特征
### 基于业务和领域知识

### 特征组合

### 基于模型

### 自动化特征工程工具
