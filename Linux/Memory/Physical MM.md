## 概述
1. 用于跟踪和管理系统中物理内存的使用情况，包括内存的分配和释放。
2. 核心任务是将物理内存划分成一系列的页面，以便可以更加高效地管理内存。
## 内存架构
现代计算机系统的内存架构主要分为两种：统一内存访问（UMA）和非统一内存访问（NUMA）
### UMA
所有 CPU 访问任何物理内存位置的延迟都是相同的。这种模型常见于单插槽的桌面或小型服务器系统 。
### NUMA架构
1. 随着多插槽、多核心服务器的普及，NUMA 架构成为主流。
2. 物理内存被划分为多个节点，每个节点都与一个或一组特定的 CPU 在物理上更近。
3. CPU 访问其“本地”节点内存的速度显著快于访问连接到其他CPU的内存。
### pg_list_data
1. 为了屏蔽底层硬件差异，提供统一编程模型，Linux 引入了 pg_list_data 结构体来抽象“内存节点”这一概念。
2. 在 NUMA 系统中，每个物理内存节点都对应一个 pg_data_t 实例。
3. 在 UMA 系统中，内核会创建名为 contig_page_data 的静态 pg_data_t 结构来管理所有内存。
## 内存区域
1. 由于硬件限制，并非所有物理内存页都是等同的。因此，在每个 pg_data_t 节点内部，物理内存被进一步划分为多个逻辑上的 区域。
2. 区域将具有相似属性或硬件约束的物理页组合在一起进行管理。
### 主要内存区域（32位x86）
1. ZONE_DMA：包含能够用于直接内存访问的物理页。主要针对一些老旧的 ISA 设备，它们只能对物理地址空间的前 16MB 进行 DMA 操作。   
2. ZONE_NORMAL：被称为“低端内存”。\
（1）是内核可以建立永久映射并直接访问的常规内存区域，通常在 16MB 到 896MB 之间。\
（2）许多关键的内核操作都依赖于此区域，因此它是性能最敏感的区域。   
3. ZONE_HIGHMEM：被称为“高端内存”，指物理地址在 896MB 以上的内存。\
（1）由于 32 位内核的虚拟地址空间有限，无法将所有物理内存都永久映射到内核地址间，因此访问高端内存需要建立特殊的临时映射。\
（2）在 64 位系统上，由于其巨大的虚拟地址空间可以轻松映射海量物理内存，ZONE_HIGHMEM 的概念已基本不再适用。
### 内存区域演变
随着技术的发展，区域的定义也从纯粹反映硬件限制演变为包含策略性用途 
1. ZONE_DMA32：为那些只能寻址 32 位地址空间的现代设备（如某些PCI设备）提供 DMA 内存。   
2. ZONE_MOVABLE：一个策略性区域。\
（1）主要用于存放那些可以被移动的内存页，例如大多数用户空间的匿名页和页缓存。\
（2）将可移动页与不可移动页（如内核数据结构）物理上隔离开来，极大地提高了内存规整的效率，从而更容易获得大块的连续物理内存，这对于透明大页等特性至关重要。   
（3）页缓存：为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间就是页缓存
3. ZONE_DEVICE：用于管理附加在外部设备上的内存，例如 GPU 显存或持久性内存（PMEM），使其能够被纳入内核的内存管理框架。
## 整合
1. 每个 NUMA 节点都通过其 pg_data_t 结构中的 node_zones 数组成员来管理自己的一组区域。
2. 并非每个节点都会包含所有类型的区域，具体取决于系统的物理内存布局、架构限制和内核启动参数。
3. 这种节点到区域的层次结构，构成了精密的分配请求过滤器，确保了最受限的内存资源被用于最关键的需求。

## 伙伴系统（Buddy System）
1. 物理内存管理的核心，负责管理区域内的空闲物理页帧。
2. 核心思想是以 2 的幂次方大小为单位来管理和分配连续的物理页块。
3. 这些块的大小对应一个阶，从 order-0（1 个页，通常为 4KB）到 order-10（ $2^{10}=1024$ 个页，即 4MB）甚至更高。
4. 内核为每个阶都维护一个空闲块链表。
### 分配过程（分裂）
当内核需要分配一个 $2^k$ 个页大小的内存块时，其过程如下
1. 查找空闲块：分配器首先检查阶为 $k$ 的空闲链表。如果链表不为空，就直接取出一个块，标记为已分配，并返回给请求者。
2. 分裂：如果阶为 $k$ 的链表为空，分配器会查找更高一阶（$k+1$）的空闲链表。如果找到一个空闲块，就将其分裂成两个大小相等、地址相邻的“伙伴”块。
3. 分配与归还：其中一个伙伴块被分配出去，另一个则被添加到阶为 $k$ 的空闲链表中，以备后续使用。
4. 递归分裂：如果阶为 $k+1$ 的链表也为空，分配器会继续向更高阶（$k+2$,...）查找，直到找到一个可用的空闲块进行递归分裂，最终产生一个阶为 $k$ 的块。
### 释放过程（合并）
当一个内存块被释放时，伙伴系统会尝试将其与它的伙伴合并，以形成更大的连续空闲块
1. 查找伙伴：当一个阶为 $k$ 的块被释放时，分配器会通过一个简单的地址计算来找到它的唯一伙伴。
2. 伙伴块的地址可以通过将被释放块的地址与块大小（$2^k \times \text{PAGE\_SIZE}$）进行按位异或（XOR）运算得到。
3. 检查与合并：分配器检查这个伙伴块是否也处于空闲状态。如果是，就将这两个伙伴块从阶为 $k$ 的空闲链表中移除，合并成一个阶为 $k+1$ 的大块。
4. 递归合并：这个新合并的块自身也有一个伙伴。分配器会重复上述过程，继续尝试与更高阶的伙伴合并。直到遇到一个已被占用的伙伴或者达到最高阶为止。
### 碎片问题
1. 优点：能非常有效地对抗外部碎片。\
（1）外部碎片指的是存在足够总量的空闲内存，但由于它们不连续，无法满足一个需要大块连续内存的请求。\
（2）伙伴系统通过在释放时积极地、递归地合并空闲块，倾向于重新组合出尽可能大的连续内存块，从而缓解了这个问题。
1. 缺点：会引入内部碎片。\
（1）内部碎片指的是分配出的内存块大于实际请求大小而造成的浪费。由于伙伴系统只能分配$2$的幂次方大小的块，任何非$2$的幂次方的请求都必须向上取整。\
（2）例如，在一个页大小为 4KB 的系统上，一个需要 33KB 内存（即9个页）的请求，最终会被分配一个16个页（64KB）的块，从而浪费了 7 个页（28KB）的空间。
### ZONE_MOVABLE
1. 系统长时间运行后，不可移动的内核分配（如某些驱动程序的缓冲区）会散布在物理内存中，使得即使存在大量空闲页，也无法形成大的连续块 。
2. ZONE_MOVABLE 的出现，使得内核可以将这些可移动的页优先分配到物理上独立的区域，与Z ONE_NORMAL 中不可移动的内核分配隔离开来。
3. 得内存规整算法可以在 ZONE_MOVABLE 上高效运行。

## Slab层
### 引出
直接使用伙伴系统分配小对象存在两个核心问题
1. 伙伴系统的最小分配单元是一个物理页（通常是 4KB）。而内核中许多数据结构大小远小于一页，会导致巨大的内存浪费。
2. 内核对象在分配后通常需要进行初始化。频繁地分配、初始化、释放、销毁这些对象，其计算开销相当可观，有时甚至超过了分配内存本身的成本。
3. 为了解决这个问题，Linux 在伙伴系统之上构建了一个专门用于内核小对象管理的高速缓存层——Slab分配器。
### 功能
1. 通过将多个相同大小的小对象“包装”在一个或多个连续的物理页（称为一个 “slab”）中，从而消除了内部碎片。
2. 通过缓存已经初始化过的对象，将对象的分配和释放操作简化为高效的链表操作，避免了重复的初始化和销毁开销，实现了“对象重用”。
### SLAB
1. 最早的实现，为每种类型的对象创建一个“缓存”，每个缓存管理着多个 slab。
2. 为了提升性能和降低锁竞争，它维护了复杂的 slab 队列（如 slabs_full、slabs_partial、slabs_free）和每CPU对象缓存。
3. 还引入了“slab 着色”技术，通过在 slab 内为对象设置不同的起始偏移，来分散对象在硬件高速缓存中的位置，以减少缓存行冲突，提升缓存利用率。
4. 随着CPU核心数的急剧增加，SLAB 复杂的元数据管理和锁机制成为了其自身的可扩展性瓶颈。
### SLUB
1. 极其简化的分配器，专为内存占用极小的嵌入式系统设计。
2. 采用简单的“首次适应”算法在全局的块链表上进行分配，SLOB 的代码量和元数据开销极小。
3. 代价是可扩展性差，且容易产生严重的外部碎片。
4. 目前 SLOB 已被标记为废弃，并由一个轻量级的 SLUB 配置选项 SLUB_TINY 所取代。
### SLOB
1. 在拥有数十甚至数百个核心的系统上，性能的主要瓶颈已从单个核心的 CPU 缓存效率转向了核心间的锁竞争和元数据同步开销。
2. 关键简化在于取消了每个 slab 内嵌的元数据头部，将 slab 管理所需的关键信息（如指向空闲对象链表头部的指针 freelist，已分配对象的计数 inuse等）存储在构成该 slab 的物理页所对应的 struct page 结构体中。
3. struct page 是内核中用于描述每个物理页帧的全局数据结构。通过这种方式，slab 本身成为一个“纯净”的对象容器。
4. 在SLUB中，一个slab内的所有空闲对象被组织成一个简单的单向链表，next 指针直接存储在空闲对象自身的内存空间里。当分配一个对象时，只需从链表头部取下一个节点；释放时，则将对象重新插入链表头部。
5. 保留了高性能的每 CPU 缓存机制，以避免在常见分配路径上的锁竞争。
6. 具备合并相似大小对象缓存的能力，进一步减少了系统中的缓存数量和内存碎片。