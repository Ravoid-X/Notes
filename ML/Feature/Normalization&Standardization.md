## 归一化 Normalization
将数据缩放到一个固定的区间内，通常是 [0,1] 或 [−1,1]
$$x_{new} = \frac{x - x_{min}}{x_{max} - x_{min}}$$
1. 适用于数据的分布范围已知且相对稳定
2. 对输入范围敏感的算法中，如Sigmoid 和 Tanh，能确保输入数据落在激活函数的有效区间内，从而加速收敛
3. 缺点是对离群值非常敏感，会丧失区分度
## 标准化 Standardization
让数据服从标准正态分布，即均值为 0、标准差为 1
$$x_{new} = \frac{x - \mu}{\sigma}$$
1. 绝大多数机器学习算法都适用标准化，因为它不依赖于特定的数据分布，而是将数据转换为一个通用的标准形式。
2. 像线性回归、逻辑回归、支持向量机 (SVM) 和 K-Means 等算法都假定数据大致服从正态分布，或者在处理标准化数据时效果更好。