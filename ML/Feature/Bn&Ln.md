## Batch Normalization
1. 对一个批次，在单个特征的所有样本上进行归一化。
2. 适用于大批量训练。在大批量下，均值和方差的估计更准确。
3. 在训练时使用批次均值和方差，在推理时使用训练过程中累积的全局均值和方差。
4. 高度依赖批量大小，小批量下效果差，且训练和推理行为不一致。
### 内部协变量偏移
在深度神经网络中，每一层的参数更新都会影响下一层的输入分布，这种输入分布的持续变化就是“内部协变量偏移”，会带来以下问题
1. 训练变慢：每一层都需要不断适应新的输入分布，导致模型需要花更长的时间才能收敛。
2. 对学习率敏感：需要使用较小的学习率来避免训练过程中的震荡和发散。
3. 梯度消失或爆炸：非线性激活函数（如 Sigmoid）在输入过大或过小时，梯度会变得很小或很大，导致梯度消失或爆炸。
### 计算过程
通常放在卷积层或全连接层之后、非线性激活函数之前。
1. $\epsilon$ 是一个很小的常数（为了防止分母为零），在 PyTorch 中默认为 $10 ^{−5}$
$$\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}$$
2. 引入两个反向传播中学习的参数：缩放因子 $\gamma$ 和平移因子 $\beta$，允许模型自主地调整标准化后的数据，使其不再严格局限于标准正态分布，从而恢复其原始特征
$$y_i = \gamma \hat{x}_i + \beta$$
### 优点
1. 可以使用更大的学习率，训练过程更加稳定，极大提高了训练速度。
2. 可以将bias置为0，因为 BN 会移除直流分量，所以不再需要 bias。
3. 对权重初始化不再敏感，通常权重采样自 0 均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了 BN 后，对与同一个输出节点相连的权重进行放缩，其标准差 σ 也会放缩同样的倍数，相除抵消。
4. 对权重的尺度不再敏感，理由同上，尺度统一由 γ 参数控制，在训练中决定。深层网络可以使用 sigmoid和 tanh 了，理由同上，BN抑制了梯度消失。
5. 具有某种正则作用，不需要太依赖 dropout ，减少过拟合。
## Layer Normalization
1. 一种用于加速训练和稳定网络的技术，尤其是在循环神经网络（RNN）和自然语言处理（NLP）模型中。
2. 对单个样本，在单个层内的所有特征上进行归一化。
3. 适用于小批量训练，或批量大小可变的情况（如 RNN/NLP）。
4. 在训练和推理时，都使用单个样本的均值和方差，所以无差异。
5. 如果不同输入特征不属于相似的类别（比如颜色和大小），可能会降低模型的表达能力。
### 计算过程
1. 计算均值和方差
$$\mu = \frac{1}{d} \sum_{i=1}^{d} h_i$$
$$\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (h_i - \mu)^2$$
2. 归一化
$$\hat{h}_i = \frac{h_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$
3. 缩放与平移
$$y_i = \gamma \hat{h}_i + \beta$$
