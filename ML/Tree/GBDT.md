## 引言
1. 梯度提升决策树（Gradient Boosting Decision Tree）核心思想是构建一个由多棵弱学习器组成的加法模型。
2. 隶属于集成学习中的“提升”（Boosting）范式，个体学习器之间存在强烈的依赖关系，必须以串行方式依次生成。
3. 串行的是指每一棵新树都旨在纠正其前面所有树累积的预测错误。
## 原理
### 梯度下降
1. GBDT的训练过程不能简单地停留在“拟合残差”，其更深的理论基础，是将整个提升过程看作是“函数空间上的梯度下降迭代” 。
2. GBDT的目标是找到一个近似函数 F(x) ，以最小化训练集上的损失函数的平均值。每一步迭代，GBDT都会寻找一个弱学习器，使其能够沿着当前模型损失函数的负梯度方向进行优化。
### 损失函数与负梯度的拟合机制
1. 其能够支持多种损失函数，只要是可微分的。
2. 每一次迭代中，新训练的决策树都会被用来拟合当前模型在每个样本点上的损失函数负梯度，而不是直接去拟合原始目标值。
## 算法流程
### 初始化
构建一个初始的弱学习器 $f _0(x)$。对于回归问题，这通常是一个常数函数，其值等于训练集目标变量的均值。
### 计算负梯度
对于第 m 次迭代，针对每个样本 i，计算当前模型 $F _{m−1}(x _i)$ 在损失函数上的负梯度，通常称为“伪残差” 。这个负梯度，记为 $g _i$，代表了当前模型在样本 i 上的“误差”方向。
### 拟合弱学习器
训练一个新的弱学习器（通常是一棵CART回归树） $h _m(x)$，使其能够拟合上一步计算出的所有样本的伪残差 $g _i$。
### 更新模型
将新学习器 $h _m(x)$ 的预测结果乘以一个学习率 ν（也称为步长），加到当前模型上，得到新的集成模型 $F _m(x _i)=F _{m−1}(x _i)+v*h _m(x)$
### 重复迭代
重复步骤2到4，直到达到预设的迭代次数 M 或模型的误差足够小 。
### 集成
最终的强学习器是所有弱学习器的累加和$F _M(x)=\sum _{m=0} ^Mf_m(x)$
## 弱学习器
通常是深度较小的决策树，尤其是分类回归树（CART），优点如下
1. 非线性建模能力: 决策树能很好地捕捉数据中的非线性关系和特征之间的复杂交互作用。
2. 鲁棒性: 它们对输入特征的量纲不敏感，无需进行特征标准化，也能很好地处理字段缺失的数据。
3. 自动化: 决策树在构建过程中能够自动进行特征选择和降维。
4. GBDT模型具有更强的可解释性，通过分析每棵树的结构和特征重要性，可以直观地理解模型是如何做出预测。
## 超参数
影响模型性能最大的三个超参数是树的数量（n_estimators）、学习率（learning_rate）和树的最大深度（max_depth）
### 树的数量
默认值为100。增加树的数量可以不断减少训练集上的误差，然而将其设置得过高可能会导致模型过度拟合。
### 学习率
1. 默认值为0.06。控制着每一棵新树对总模型的贡献大小。
2. 经验表明，使用较小的学习率（例如低于1.0）能够显著提高模型的泛化能力。但这也意味着需要更多的树来达到相同的训练效果。
3. 当学习率减小时，为了弥补每棵树对模型的贡献降低，通常需要增加树的数量。
### 树的最大深度
1. 默认值为1。树的深度越大，其对数据的拟合能力越强，模型精度通常也越高。然而，过高的深度会使模型过于复杂，容易导致过拟合。
2. 在实践中，对于样本量多、特征也多的情况，通常会限制最大深度，具体的取值取决于数据的分布，通常在10-100之间。
## 子采样（Subsample）
1. 在每一步迭代中，只使用训练集的随机子样本来拟合新的弱学习器。
2. 这个子样本通常是原始训练集的一个常数比例（例如在0.5到0.8之间）。这种子采样方式能够有效减少模型的方差，进而防止过拟合，但可能会增加偏差。
3. 使用了子采样的GBDT有时也称为随机梯度提升树（Stochastic Gradient Boosting Tree, SGBT）。
4. 通过这种方法，GBDT在一定程度上获得了与随机森林相似的随机性，从而增强了模型的健壮性。  
## 与随机森林比较
1. 训练模式: GBDT 的树只能串行生成，随机森林的树可以并行生成，这使其在大数据时代具有显著的计算效率优势。   
2. 结果聚合: GBDT 通过将所有树的预测结果累加起来得到最终结果。而随机森林则采用多数投票（分类）或简单平均（回归）的方式来聚合结果。   
3. 性能侧重: 随机森林通过在训练过程中引入随机性来降低模型的方差，从而提高模型的泛化能力。而 GBDT 则主要通过迭代修正来降低模型的偏差。   
4. 对异常值敏感性: 随机森林由于其并行和聚合的特性，对异常值不敏感。而GBDT则对异常值非常敏感，因为异常值会产生巨大的残差，迫使后续的树去过度拟合这些异常点，从而影响模型的健壮性。