## 引出
一种非参数监督式学习算法，本质上是一系列“if-else”条件控制语句来展示算法的结构化方式。
## 结构
1. 根节点： 位于树的最顶端，没有传入分支。代表整个数据集，并包含着第一个需要评估的特征问题。   
2. 内部节点 (Internal Nodes)： 也称为决策节点。代表对某一特定特征的评估，并根据评估结果引出不同的分支。将数据进一步划分为更小的同质子集。   
3. 分支 (Branches)： 连接节点，代表了评估结果的可能选项或选择路径。   
4. 叶子节点： 也称为终端节点，没有引出分支。代表了数据集所有可能的最终结果或分类标签。
# 优缺点      
## 优点
1. 易于理解和解释。   
2. 对数据准备要求低： 无需进行复杂的数据预处理（如归一化）就可以同时处理数值型和分类型数据。此外，某些决策树算法（如 C4.5 和 CART）能自然地处理缺失值。   
3. 对异常值稳健： 构建过程对异常值不敏感，因为预测是基于数据子集上的聚合函数（如均值或众数）来生成的。   
4. 非参数化： CART 等决策树算法不对数据的底层分布做出任何假设。   
## 缺点
1. 容易过拟合： 单棵决策树最显著的缺点。如果不对其生长进行限制，它会变得过于复杂，过度地学习训练数据中的噪声和细节，从而在新数据上表现不佳。   
2. 高方差： 数据中的微小变化可能导致决策树结构发生巨大改变，从而产生不稳定的预测结果。   
3. 贪心算法： 决策树在每个节点上都做出局部最优的分割决策，不保证能够达到全局最优的树结构。   
4. 非连续性： 回归树的预测是阶梯状的，无法平滑地拟合连续函数，这使得它在处理线性关系数据时不如线性回归模型。  
# 树的生长 
## 理论基础
1. 决策树构基于贪婪的、自顶向下的递归分区策略，目标是使每个子节点中的数据比其父节点更“纯净”或更“同质”，即节点内的大多数数据点都属于同一个类别。
2. 衡量一个节点数据混乱程度的指标称为“不纯度” 。决策树算法目标是找到能最大化不纯度降低的分割点。
3. 熵（Entropy）用于量化数据集中的不确定性。一个低熵值表示数据集更加纯净，即其不确定性较小。
## 基尼不纯度
1. 计算公式：$Gini(S) = 1 - \sum_{i=1}^{c} p_i^2$，$p _i$ 是数据点属于类别 i 的比例
2. 度量从一个数据集中随机抽取的一个数据点被错误分类的概率
3. 对于二元分类，取值范围在 0 到 0.5 之间。值为 0 表示完全纯净的分割；值为 0.5 表示最大不纯度，即两个类别的混合程度最高。
4. 对于多类别分类，基尼不纯度的最大值为 $1-\frac 1 C$，C 为类别总数
## 信息增益
决策树用来决定哪个特征是“最佳”分割点的核心准则，量化了在某个特征上进行分割后，不纯度减少的程度 。 
$$Inf \ Gain(S, A) = E(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} E(S_v)$$
S：父数据集；A：用于分割的属性；Values(A)：属性 A 的所有可能值集合。
$S _v$​ ：数据集中属性 A 的值为 v 的子集；$|S|∣$ 和 $|S _v|∣$：分别是数据集 $S$ 和 $S _v$ 的数据点数量。
## 信息增益率
信息增益作为分裂准则有一个固有的缺陷：它倾向于选择那些取值数量较多的特征进行分裂。信息增益率将信息增益除以特征本身的熵，从而惩罚那些取值过多的特征，避免模型过度特例化。
# 剪枝（Pruning）
算法旨在最大限度地拟合训练集时，可能会导致树的深度过大，叶节点过于纯净，从而学习到训练数据中的“噪声”。剪枝通过移除决策树中分辨能力较弱的节点，降低模型的复杂度，从而降低过拟合的风险。
## 预剪枝
1. 在决策树完全生长之前，就提前停止其生长 。一种“边建树边剪枝”的策略。
2. 实现方法\
（1）限制树的最大深度\
（2）限制叶节点所需的最小样本数\
（3）设置信息增益阈值，如果分裂带来的信息增益小于此阈值，则停止分裂
3. 优点：速度快、计算成本低。
4. 缺点：可能导致“欠拟合”，提前停止分裂，可能会错过当前看来不重要但在后续会变得重要的分支，称为“视界限制效应”。
## 后剪枝
1. 先从训练集生成一棵完整的决策树，直至达到预设的停止条件（如每个叶节点仅含少数样本），然后再自底向上地对非叶节点进行考察和剪除。
2. 通过参数 α 来权衡模型的复杂度（树的大小）和训练误差 。如果移除一个子树后，模型的泛化性能在交叉验证下没有降低，则将其剪除。
3. 优点：通常能获得比预剪枝更好的泛化性能
4. 缺点：计算成本更高，因为需要先生成完整的树，再进行额外的剪枝
# 算法
## ID3（Iterative Dichotomiser 3）
1. 核心思想是利用信息增益作为唯一的特征选择标准，以贪心方式递归地构建树。
2. 在每一步，遍历所有特征，选择能带来最大信息增益的特征进行分裂，直到所有节点上的数据都属于同一类别。
3. 无法直接处理连续值数据，必须先将其离散化才能应用。
4. 由于其依赖信息增益，会偏向于选择取值数量多的特征作为分裂节点，容易导致过拟合，这使得它在处理某些特定类型的数据时表现不佳。
## C4.5
被视为ID3的“后期迭代版本”，核心改进点包括
1. 引入信息增益率：解决了 ID3 偏向选择取值多特征的问题。   
2. 处理连续属性：能够通过设置阈值将连续值转换为分类值，从而直接处理连续数据。   
3. 处理缺失值：引入了对带有缺失属性值的训练数据进行处理的机制。   
4. 支持剪枝：在构建树后，C4.5 能够对树进行剪枝，以降低模型的复杂度并防止过拟合。
## CART
1. 与ID3和C4.5不同，其只能生成二叉树。
2. 对于分类任务，CART通常使用基尼不纯度来选择最佳分裂属性。
3. 对于回归任务，它则使用方差或均方误差来衡量不纯度。
4. 统一分类和回归，是集成学习基石。