## 引言
1. 集成学习是一种将多种机器学习算法的预测结果结合起来，以期获得比任何单一算法都更优性能的方法。
2. 核心理念是整合大量独立的、表现各异的决策树，以某种集体决策机制（如多数投票或平均值）来形成一个最终的、具有卓越泛化能力的“强”模型。
3. 随机森林中的每一棵树都被称为“基学习器” 。通常被允许生长得非常深，甚至不进行剪枝，以便能够充分拟合各自的训练数据，从而确保具有较低的偏差。
4. 随机森林成功的关键在于其随机性，确保了每棵树之间的相关性较低，从而有效降低方差风险和过拟合问题。
## 随机化机制
### 自助采样（Bagging）
1. 在训练每一棵决策树时，采用有放回的均匀抽样方法，从原始数据集中随机抽取一个大小相等的子集 。这种方法被称为自助聚合（Bootstrap Aggregating），简称Bagging。
2. 未被选中的样本（大约占原始数据集的37%，即$(1 - 1/d)^d \approx 1/e \approx 0.368$）被称为“袋外”数据（Out-of-Bag，简称OOB），在后续环节作为一种内部验证机制.
3. 无偏误差估计（OOB Score）：OOB数据可以作为测试集，对相应的树进行验证。对所有树的OOB预测结果进行汇总，得到一个对模型泛化能力的无偏估计。
### 随机子空间
1. 进行节点分裂时，不会在所有可用的特征中寻找最佳分裂点，而是随机选择一个特征子集，然后仅在该子集内部寻找最佳分裂。
2. 有效防止了数据集中少数几个具有强大预测能力的特征主导所有决策树的构建。
## 类权重调整（Class Weighting）
给少数类分配更高的权重，而给多数类分配更低的权重时，算法在训练过程中会更加关注少数类样本 。
## 预测流程
### 分类任务
采用“多数投票”机制。每棵树都会对输入样本进行分类，然后算法统计所有树的投票结果，将票数最多的类别作为最终预测结果 。   
### 回归任务
最终结果是所有决策树预测值的简单平均值。   
## 偏差与方差
1. 一个未经剪枝的深层决策树能够精确拟合训练数据，包括噪声。这导致其偏差很低，但对训练数据的微小变动极其敏感，因此方差很高。
2. 核心贡献在于其通过集成策略显著降低了模型的方差，而对偏差的影响微乎其微。
## 优点
1. 不容易过拟合，泛化能力强
2. 能够高效处理包含高维数据，并且能够自动评估和排序变量的重要性
3. 训练速度比较快，容易做成并行方法
4. 对缺失值和异常值不敏感
## 缺点
1. 构建森林消耗大量的计算和资源
2. 单个决策树易于理解和解释，但随机森林是一种“黑箱”模型
3. 被证明在某些噪音较大的分类或回归问题上会过拟合
4. 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以在这种数据上产出的属性权值是不可信的