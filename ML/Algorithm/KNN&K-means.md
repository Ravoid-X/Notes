# KNN
## 原理
1. 监督学习算法，一种基于实例的学习或局部近似学习方法。没有显式的训练过程，而是将所有计算推迟到分类或预测阶段，因此又被称为“惰性学习”算法。
2. 不构建任何通用模型，而是简单地将整个训练数据集存储在内存中。当需要对一个新数据点进行预测时，它才开始进行计算，并根据其K个最近邻居的信息做出决策。
## 算法流程
### 参数设定
设定两个关键参数：要考虑的最近邻居数量 K 和一个距离度量标准。 
### 距离计算
对于一个新的数据点（测试样本），计算它与训练数据集中所有样本的距离，这一步是算法的核心。   
### 邻居选取
在计算出所有距离后，找出与测试样本距离最近的 K 个训练样本 。   
### 决策生成
最后，基于这 K 个最近邻居的标签或值，对测试样本进行预测。
1. 分类任务：采用“多数投票”或“众数”机制 。   
2. 回归任务：计算平均值来做出预测。更高级的方法是使用加权平均，根据邻居与测试样本的距离赋予不同的权重，通常是距离越近的邻居权重越大。   
## 参数选择
### K
1. K值过小：模型复杂度高，容易“过拟合”训练数据，方差高而偏差低。
2. K值过大：模型复杂度降低，偏差高而方差低。可能导致“欠拟合”或误分类 。   
3. 选择方法：最有效的方法是交叉验证法，通过将数据集分成多份，轮流使用其中一份作为验证集，并尝试不同的K值，最终选择在验证集上表现最好的K值。
### 距离度量
1. 特征连续：选用曼哈顿距离（L1距离）/欧氏距离（L2距离）
2. 特征离散：汉明距离
## 优点
1. 适合处理稀有事件分类和多分类方面的问题，效果优于SVM
2. 简单易实现，超参数少
3. 由于没有训练阶段，添加新训练样本时，可以立即调整其预测以包含这些新数据。
## 缺点
1. 计算和存储成本高，必须存储整个训练数据集，且每次进行预测时都需要计算与所有训练样本的距离。
2. 样本不平衡时会影响到分类结果
3. 特征维度极高时，数据变得极其稀疏，所有样本点之间的距离差异变小，难以区分。
# K-means
## 原理
1. 无监督聚类算法，核心思想是通过迭代，使得同一簇内的样本点尽可能相似，而不同簇间的样本点尽可能不同。
2. 最小化数据点与其所属簇的质心（Centroid）之间的距离之和来实现上面目标。质心代表了该簇所有数据点的平均位置。
3. 最终聚类结果可能对初始质心的选择非常敏感。
## 算法步骤
### 初始化
1. 指定聚类的数量 K 后，算法随机选择K个数据点作为初始的簇质心。
2. 为了提高初始化质量，更优的策略如 K-Means++，会选择距离已选质心较远的点作为下一个质心，从而避免随机选择的局限性。   
### 分配步骤（E-step）
计算数据集中的每个点与所有 K 个质心之间的距离，并将每个点分配到距离其最近的质心所在的簇 。   
### 更新步骤（M-step）
分配结束后，根据每个簇中所有数据点的均值来重新计算新的质心 。   
### 迭代与收敛
重复执行“分配”和“更新”这两个步骤，直到簇的分配不再发生变化，或者达到预设的最大迭代次数为止。
## K 设定
通常采用以下两种主要方法
### 手肘法（Elbow Method）
随着K值的增加，簇内误差平方和（SSE）会逐渐减小。当 K 值增加到某个点后，SSE 下降速度会显著放缓，这个“拐点”通常被视为最佳的 K 值 。   
### 轮廓系数法（Silhouette Coefficient）
1. 轮廓系数 S 值在 -1 到 1 之间，值越高表示聚类效果越好，即簇内紧密且簇间分离。
2. $s(i) = \frac{b(i)-a(i)}{\max(a(i),b(i))}$\
$a(i)$ 是样本点 i 与其所在簇内所有其他点之间的平均距离\
$b(i)$ 是样本点 i 与距离最近的簇中所有点之间的平均距离
3. 不同的 K 值进行聚类，计算每种聚类结果的平均轮廓系数，最后选择平均轮廓系数最大的那个 K 值。
## 优点
1. 原理简单实现容易，计算速度快，适用于处理大规模数据集 。
2. 当簇近似高斯分布的时候，效果非常不错
## 缺点
1. K-Means 假设数据簇是各向同性且呈球形或凸形分布。当数据集是非凸形或各簇的方差不相等时，聚类效果会很差，因为会错误地将非球形簇切分。
2. 由于质心是均值，离群点会显著影响其位置，可能导致一个不重要的离群点形成一个单独的簇，或者使一个簇的质心偏离其真实中心。
3. 对初始的簇中心敏感
4. 样本只能归为一类，不适合多分类任务。
5. 不适合太离散的分类、样本类别不平衡的分类的分类。