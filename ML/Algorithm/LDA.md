# 线性判别分析（Linear Discriminant Analysis, LDA）
## 概述
1. 监督学习方法，该技术具有双重功能：既可以作为一个具有线性决策边界的分类器，也可以作为一种有效的降维技术。
2. 核心思想是投影后，类内方差最小，类间方差最大。
3. LDA 降维得到的特征，不仅是原始数据的压缩表示，更是数据中最强大的“判别信号”，这使得其在分类问题中具有高度的可解释性。   
## 数学基础
### 费雪准则（主要目标）
1. 定义：投影后数据的类间散度与类内散度之比。
2. 公式 
$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$
$S _B$ 是类间散度矩阵（Between-Class Scatter Matrix），$S _W$ 是类内散度矩阵（Within-Class Scatter Matrix）
3. 目标就是找到一个投影向量 $w$，使得 $J(w)$ 最大化
### 类内散度矩阵 $S _W$
1. 衡量类别内部的聚合度，应最小化
2. 先计算每个类别的协方差矩阵，然后相加。
3. 公式
$$S_W = \sum_{i=1}^{C} S_i = \sum_{i=1}^{C} \sum_{x \in \omega_i} (x - \mu_i)(x - \mu_i)^T$$
### 类间散度矩阵 $S _B$
1. 衡量不同类别中心之间的离散程度，应最大化
2. 计算每个类别的均值与整个数据集的总体均值之间的距离，并由每个类别中的样本数量进行加权 。
3. 公式
$$S_B = \sum_{i=1}^{C} N_i(\mu_i - \mu)(\mu_i - \mu)^T$$
### 广义特征值
1. 对 $J(w)$ 关于 $w$ 求导，并令其等于零。这个最优化问题可以转化为一个广义特征值问题：
$$S_B w=\lambda S_W w$$
2. 通常，这个问题通过求解矩阵 $S_W^{-1}S_B$ 的特征值和特征向量来解决
3. 特征向量就是线性判别向量，而 $\lambda$ 则代表了在该方向上类间散度与类内散度的比值。
4. 求解 $S_W^{-1}S_B$ 的特征向量，本质上等同于在通过类内协方差进行白化处理后的数据上，对类别的均值执行主成分分析（PCA）。
### 多类别问题
1. 线性判别向量由 $S_W^{-1}S_B$ 的前 k 个最大特征值所对应的特征向量构成
2. 其中 k 是新空间的维度，且 k≤min(C−1,p)，p 是原始特征的数量 。   
## 算法流程
### 数据预处理与标准化
1. 假设有 N 个样本，每个样本有 d 个特征，同时被标记为 C 个类别。
2. 将所有样本构成一个矩阵 X，其中每一行表示一个样本，第 j 列表示该样本的第 j 个特征。对应的类别标签构成向量 y
3. LDA 的目标函数是比率形式，对特征尺度的敏感性低于 PCA，但将数据进行标准化仍然是一个推荐的最佳实践。   
### 计算各类别均值向量
针对每个类别 i，计算其均值向量 $\mu _i$。$\mu _i$ 的第 j 个元素表示在第 j 个特征上属于类别 i 的样本的平均值。   
### 计算类内与类间散度矩阵
使用上述的公式计算
### 对 $S_W^{-1}S_B$ 矩阵进行特征分解
1. 这是 LDA 算法的计算核心。在实践中，一个常见的问题是当特征数量 p 大于样本数量 n 时，$S_W$ 可能会是奇异矩阵（不可逆）。
2. 可以采用正则化技术解决，例如“收缩”（Shrinkage），通过向 $S_W$ 的对角线添加一个小的扰动值来使其可逆。
### 选择判别向量并构建转换矩阵
1. 将特征向量根据其对应的特征值进行降序排序。
2. 选择前 k 个特征向量（其中 k 是目标子空间的维度，且 k≤C−1）作为列向量，构建一个 d×k 维的转换矩阵 W。
### 将数据投影到低维子空间
使用转换矩阵 W，将原始的 n×d 维数据矩阵 X 转换到一个新的 n×k 维矩阵 Y。这矩阵 Y 就是原始数据在低维判别空间中的表示。
## 模型的关键统计假设
### 多元正态性（高斯分布）
假设每个类别中的预测变量都服从一个多元正态分布。   
### 同方差性（协方差矩阵相等）
1. 所有类别共享同一个协方差矩阵，正是这个假设决定了 LDA 的决策边界是线性的。
2. 如果该假设被严重违反，二次判别分析可能是更合适的选择，因为它允许每个类别拥有自己独立的协方差矩阵。   
### 线性可分性与特征独立性
假设类别是线性可分的。此外，当特征之间不存在多重共线性时，LDA 表现最好，因为高度相关的特征会使类内散度矩阵 $S_W$ 变得不稳定或奇异。
## 优点
### 降维效果好且能解决分类问题
通过学习类别之间的差异来选择合适的投影方向，使得同一类别样本之间的距离尽可能小，不同类别样本之间的距离尽可能大。
### 抗噪性强
通过学习类别之间的差异来确定投影方向，能够部分抵抗数据中的噪声。
## 局限性
### 对异常值敏感
因为 LDA 的计算依赖于数据的均值和协方差，少数极端值就可能严重扭曲这些统计量的估计。   
### “小样本量”（SSS）问题
当特征数量 p 远大于样本数量n  时，类内散度矩阵 $S_W$ 会变成奇异矩阵，无法求逆。这在基因组学、图像识别等高维数据领域是一个常见挑战。  
### 无法处理非线性决策边界
LDA 创建的是线性边界，对于那些最优决策边界是非线性的问题（如XOR问题、同心圆分布）将无法有效分类 。   
### 无法处理多模态数据
LDA 为每个类别拟合一个单峰的高斯分布。如果某个类别的数据本身包含多个簇（即呈现多模态分布），将无法准确地对该类别进行建模和分离。   