# 主成分分析（Principal Component Analysis, PCA）
## 概述
1. 将一组可能存在线性相关性的变量，转化为一组数量更小、彼此线性不相关的变量，这些新的变量即被称为“主成分”。
2. 将高维数据集投影到更小的特征空间中，从根本上降低了模型复杂度。
3. 不仅能最大限度地减少或完全消除多重共线性，还能保留数据中最具信息量的特征，从而提高模型的泛化性能，防止过拟合。
## 主成分原理
### 最大化方差
1. 找到原始变量的线性组合，形成新的复合变量（即主成分），并使得这些新变量的方差被依次最大化。
2. 第一个主成分被要求捕获尽可能大的方差，第二个主成分在与第一个正交的约束下捕获次大的方差，以此类推。
### 最小化协方差
1. 确保不同主成分之间的协方差为零，意味着这些新的主成分是线性不相关。
2. 消除了新特征之间的信息冗余，使得每个主成分都捕捉了数据中一个独立的变化来源。
### 原理
本质上就是通过一次坐标旋转，将协方差矩阵对角化，即让所有非对角线元素（协方差）变为零，同时将对角线元素（方差）按照大小重新排列。
## 分解
### 特征分解
协方差矩阵是一个实对称矩阵，保证了其具有实数特征值和一组正交的特征向量。\
$$Cv=λv$$
1. 特征向量作为主成分\
（1）协方差矩阵的特征向量精确地定义了数据中方差的主方向。这些向量是相互正交的，它们构成了新的坐标系的基轴。\
（2）特征向量的元素被称为“载荷”（loadings），它们描述了每个原始变量对该主成分的贡献程度或权重 。 
2. 特征值作为解释的方差\
（1）与每个特征向量相关联的特征值，量化了数据在该特征向量方向上的方差大小。特征值越大，意味着其对应的特征向量（主成分）解释了数据中越多的变异性。\
（2）因此，主成分是按照其对应特征值的大小从高到低进行排序的，第一主成分对应最大的特征值。   
### 奇异值分解
$$A=U\sum {V}^{T}$$
1. 数据矩阵 A 的右奇异向量（即 V 的列）与协方差矩阵 $X^TX$ 的特征向量是相同的 。因此，V 的列向量直接给出了主成分的方向。
2. X 的奇异值与协方差矩阵的特征值之间存在简单的关系：特征值是对应奇异值的平方 ($λ _i=σ_i^2$) 。
### 实际应用
1. 基于 SVD 的 PCA 实现通常是首选，因为 SVD 在数值计算上比先计算协方差矩阵再进行特征分解更加稳定，可以避免由于浮点数精度问题导致的计算误差。
2. 当特征数量 p 远大于样本数量 n 时，直接对 n x p 的数据矩阵 X 进行 SVD 比计算和分解 p x p 的协方差矩阵 $X^TX$ 在计算上更高效。   
## 算法流程
### 中心化与缩放
1. PCA 对变量的尺度非常敏感，如果不同特征的数值范围差异巨大，在计算方差时，数值范围大的特征将主导主成分的计算。
2. 进行 Z-score 标准化
### 计算协方差矩阵
假设只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 X
$$
X = \begin{pmatrix}
a_1 & a_2 & \cdots & a_m \\
b_1 & b_2 & \cdots & b_m
\end{pmatrix}
$$
$$
C=\frac{1}{m} \mathbf{X} \mathbf{X}^{\top} = \begin{pmatrix} \frac{1}{m} \sum_{i=1}^{m} a_i^2 & \frac{1}{m} \sum_{i=1}^{m} a_i b_i \\ \frac{1}{m} \sum_{i=1}^{m} a_i b_i & \frac{1}{m} \sum_{i=1}^{m} b_i^2 \end{pmatrix} = \begin{pmatrix} \operatorname{Cov}(a, a) & \operatorname{Cov}(a, b) \\ \operatorname{Cov}(b, a) & \operatorname{Cov}(b, b) \end{pmatrix}
$$
### 对协方差矩阵进行特征分解
### 选择主成分（降维）
1. 排序：将特征值按从大到小的顺序排列。同时，将对应的特征向量也按照相同的顺序重新排列。拥有最大特征值的特征向量就是第一主成分（PC1），第二大特征值对应的就是第二主成分（PC2），以此类推 。   
2. 选择：决定保留前 k 个特征向量（其中 k < p），并舍弃剩下的 p-k 个 。被舍弃的成分是那些对应较小特征值的，它们被假定为代表了数据中的噪声或次要信息 。   
3. 构建投影矩阵：将选出的前 k 个特征向量作为列向量，组合成一个 p x k 的矩阵。这个矩阵有时被称为投影矩阵或特征向量矩阵。   
### 数据投影（生成最终数据集）
1. 将原始的、标准化的 n x p 数据矩阵转换到新的 n x k 特征空间。
2. 变换：原始标准化数据矩阵与 p x k 投影矩阵的点积。   
3. 结果：得到的新矩阵就是降维后的数据集。该矩阵有 n 行（每个样本一行）和 k 列（每个主成分一列）。矩阵中的值被称为每个样本在对应主成分上的“得分”，表示原始样本在新坐标系下的坐标。
## 特性
1. PCA 保留了训练集的主要信息，但主要信息未必是重要信息。
2. 对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来，因为测试集不可观测所以不会知道其均值。
3. 本质上是一个有损压缩过程。
4. 每个主成分都是所有原始特征的线性组合，难以解释其含义
5. 当特征已不相关或重要信息不在高方差方向，不应使用 PCA。