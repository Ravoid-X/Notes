## Batch
每次训练迭代中，模型处理的样本数量
### 小 Batch Size (例如 16, 32)
1. 优点\
（1）引入更多的随机性，有助于模型跳出局部最优解，提高泛化能力\
（2）内存占用小，允许训练更大的模型
2. 缺点\
（1）训练过程的损失函数波动较大，收敛不稳定\
（2）可能无法充分利用 GPU 等硬件的并行计算优势
### 大 Batch Size (例如 256, 512)
1. 优点\
（1）每次更新的梯度估计更稳定，训练过程更平滑\
（2）可以充分利用 GPU 的并行计算能力，单次迭代计算效率
2. 缺点\
（1）泛化能力可能不如小 Batch Size\
（2）需要更多的显存
### 设置
从 32、64 或 128 等开始，并调整
## Epoch
模型遍历整个训练数据集的完整周期
### 设置
1. Epoch 过少：模型没有完全训练，可能导致欠拟合。
2. Epoch 过多：模型可能过拟合。记忆训练数据中的噪声和细节，导致在测试集上的性能下降。
3. 使用“提前停止”：设置一个较大的 Epoch 数，但在训练过程中持续监控模型在验证集上的性能。当验证集的损失不再下降或开始上升时，就停止训练。
4. 观察训练曲线：如果训练集和验证集的损失曲线都在持续下降，说明 Epoch 还可以继续增加，否则停止
## Iteration
一个 Batch 的训练过程，Iteration 数量 = 训练样本总数 / Batch Size