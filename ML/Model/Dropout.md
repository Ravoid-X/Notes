## 引言
正则化技术，用于防止神经网络过拟合。核心思想是在训练过程中随机地“丢弃”一部分神经元，从而阻止网络对训练数据产生过度依赖。
## 工作原理
在神经网络的每一次前向传播过程中，以一定的概率 p 随机地将某些神经元及其连接暂时“失活”（设为 0）。
### 训练阶段
1. 每次训练时，对每个神经元，以预设的失活率 p（通常 0.1-0.5）随机决定是否将其丢弃。
2. 被丢弃的神经元在本次迭代中不参与前向传播和反向传播，相当于本次训练中这个网络结构变得更“小”了。
3. 这种随机性使得每次迭代都相当于在训练一个不同的、更小的子网络。
### 测试（推理）阶段
1. 在测试时，所有的神经元都被保留，不进行 Dropout 操作。
2. 为了弥补训练时部分神经元失活的影响，需要对所有神经元的输出进行缩放。通常将每个神经元的输出乘以训练时设定的保留率（即 1−p）。
3. 如果训练时的失活率是 p=0.5，那么测试时所有神经元的输出值都需要乘以 0.5。这种缩放操作确保了训练和测试阶段，每个神经元的预期总输入保持一致
### 作用
1. 减少神经元之间的共适应性：在没有 Dropout 的网络中，某些神经元可能会“合谋”来共同处理特定的特征，导致网络对这些特定的特征组合产生强烈的依赖。
2. 集成学习：整个训练过程可以被看作是成千上万个不同子网络的集成
## 通常设置
1. 全连接层：通常设置为 0.5。能最大化神经元丢弃的随机性，既保证网络有足够的信息流，又防止过拟合。
2. 卷积层：通常使用较低的失活率，比如 0.1 到 0.2。因为卷积层本身参数共享，已经具有一定的正则化效果。
3. RNN：应用 Dropout 需要特别小心。通常只在非循环连接上应用，例如在不同时间步之间。