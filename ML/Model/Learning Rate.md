## 引出
决定了在每次迭代中，模型参数沿着损失函数梯度下降方向更新的步长
$$w_{\text{new}} = w_{\text{old}} - \eta \frac{\partial L}{\partial w}$$
## 学习率过大
1. 训练震荡：模型参数可能会在损失函数的最低点附近来回跳动，无法收敛到最优解。
2. 错过最优解：参数更新可能直接“跳过”损失函数的最低点，无法收敛。
3. 训练失败：在极端情况下，损失函数值可能会越来越大，导致训练失败。
## 学习率过小
1. 训练缓慢：模型收敛到最优解所需的时间会非常长，训练效率低下。
2. 陷入局部最优：模型可能在损失函数的“平坦区域”或次优解处停止更新
## 初值设置
对于大多数深度学习任务，一个合理的初始学习率通常 $10 ^{−2}$ 和 $10 ^{−4}$之间
1. 对于 Adam 优化器，一个好的起点是 $10 ^{−3}$
2. 对于随机梯度下降（SGD），由于它没有自适应能力，通常会使用更小的学习率如 $10 ^{−2}$ 或 $10 ^{−3}$
### 范围测试
1. 从小到大逐渐增加学习率：从一个非常小的学习率（例如 $10 ^{−8}$）开始，并在每个训练批次后按指数方式增加学习率。
2. 记录损失并寻找最佳点，最佳的初值通常是损失开始急剧下降的点，而不是最低点
## Adam
一种非常流行的自适应学习率优化算法，有两个关键组成部分，分别对应 Momentum 和 RMSprop 的思想
1. 一阶矩估计（First Moment Estimation）：类似于 Momentum，计算了梯度的一阶矩（即梯度的指数加权移动平均），能平滑梯度更新，并加速在相关方向上的收敛。
2. 二阶矩估计（Second Moment Estimation）：类似于 RMSprop，计算了梯度的二阶矩（即梯度的平方的指数加权移动平均），它能为每个参数自适应地调整学习率。
### 算法流程
#### 初始化一阶矩向量 $m_0$ 和二阶矩向量 $v_0$ 为零向量
$$m_0 = 0,v_0 = 0$$
#### 迭代计算，在第 t 次迭代中，对于每个参数 $w_t$
1. 计算当前梯度
$$g_t = \nabla_{w} L(w_{t-1})$$
1. 更新一阶矩向量（动量），$\beta_1$ 是动量的超参数，通常设置为 0.9
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
1. 更新二阶矩向量（方差），$\beta_2$ 是方差的超参数，通常设置为 0.999
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
#### 偏差校正
由于 $m_0$ 和 $v_0$ 初始化为零，在训练初期，一阶和二阶矩估计会偏向于零，引入偏差校正来抵消这个影响
1. 一阶矩偏差校正
$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
1. 二阶矩偏差校正
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
#### 参数更新
$\epsilon$ 是一个很小的常数（例如 $10 ^{−8}$），用于防止分母为零
$$w_t = w_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$
### 优点
1. 能为每个参数自动调整学习率，这使得它对超参数的设置不那么敏感。
2. 结合了动量和自适应学习率，通常比传统的 SGD 和其他优化器收敛得更快。
3. 作为一种“默认”优化器，在大多数深度学习任务中表现良好，无需过多调整。
### 缺点
1. 在某些情况下，Adam 可能比带有精心调整学习率的 SGD 具有更差的泛化性能。
2. 在某些理论研究中发现，Adam 的收敛性可能不总是最优，尽管在实践中它表现得非常好。