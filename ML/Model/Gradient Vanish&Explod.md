## 梯度消失
在反向传播过程中，梯度随着网络层数的增加而指数级减小，甚至趋近于零。这导致靠近输入层的参数几乎无法得到有效的更新
### 原因
1. 网络层次过深
2. 激活函数选择不当，比如Sigmoid 和 Tanh，导数最大值分别为0.25和 1
### 影响
1. 靠近输入层的网络层参数得不到有效更新，模型无法学习到数据的深层特征。
2. 训练变得异常缓慢，甚至停止。
### 解决
1. 使用 ReLU 激活函数
2. 使用残差连接
3. 更好的权重初始化：如 Xavier 或 He 初始化，确保梯度在网络中能保持在一个合理的范围内
## 梯度爆炸
在反向传播过程中，梯度随着网络层数的增加而指数级增大，变得非常大。这导致参数更新幅度过大，模型在训练过程中变得不稳定，甚至发散。
### 原因
1. 网络层次过深
2. 如果网络权重初始化得过大，或者在训练过程中参数变得非常大
### 影响
1. 参数更新步长过大，模型在损失函数空间中“跳来跳去”，无法收敛。
2. 损失函数的值变成 NaN（非数字），导致训练失败。
### 解决
1. 设置一个阈值进行梯度剪裁
2. 减小学习率可以有效控制参数更新的步长，防止模型因更新过大而发散
3. L1 或 L2 正则化可以限制模型参数的大小，从而间接防止梯度爆炸